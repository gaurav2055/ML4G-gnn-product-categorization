{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphSAINT on ogbn-products (500K Subsample)\n",
    "## ML4G Course Project - Scalability Model Implementation\n",
    "### Team: Abhishek Indupally, Pranav Bhimrao Kapadne, Gaurav Suvarna\n",
    "\n",
    "This notebook implements GraphSAINT with three sampling strategies:\n",
    "- Random Walk Sampling (recommended)\n",
    "- Node Sampling\n",
    "- Edge Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla P100-PCIE-12GB\n",
      "Available GPU memory: 12.78 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.loader import GraphSAINTRandomWalkSampler, GraphSAINTNodeSampler, GraphSAINTEdgeSampler\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch.serialization import add_safe_globals\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data (Consistent with Previous Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataEdgeAttr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Allowlist required torch_geometric classes for safe unpickling\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m add_safe_globals([\u001b[43mDataEdgeAttr\u001b[49m, DataTensorAttr, GlobalStorage, NodeStorage, EdgeStorage, Data, Batch])\n",
      "\u001b[31mNameError\u001b[39m: name 'DataEdgeAttr' is not defined"
     ]
    }
   ],
   "source": [
    "# Allowlist required torch_geometric classes for safe unpickling\n",
    "add_safe_globals([DataEdgeAttr, DataTensorAttr, GlobalStorage, NodeStorage, EdgeStorage, Data, Batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading ogbn-products dataset...\")\n",
    "dataset = PygNodePropPredDataset(name=\"ogbn-products\", root=\"data\")\n",
    "data = dataset[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ORIGINAL DATASET INFO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total nodes: {data.x.shape[0]:,}\")\n",
    "print(f\"Node features: {data.x.shape[1]}\")\n",
    "print(f\"Total edges: {data.edge_index.shape[1]:,}\")\n",
    "print(f\"Labels shape: {data.y.shape}\")\n",
    "print(f\"All unique labels: {torch.unique(data.y).numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 500K subsample with seed=42 (same as other models)\n",
    "def create_subsample(data, num_nodes=500000, seed=42):\n",
    "    \"\"\"\n",
    "    Create a consistent subsample of the graph.\n",
    "    This ensures fair comparison across all models.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Random sample of nodes\n",
    "    perm = torch.randperm(data.num_nodes)\n",
    "    sampled_nodes = perm[:num_nodes]\n",
    "    \n",
    "    # Create node mapping\n",
    "    node_mapping = torch.full((data.num_nodes,), -1, dtype=torch.long)\n",
    "    node_mapping[sampled_nodes] = torch.arange(len(sampled_nodes))\n",
    "    \n",
    "    # Filter edges\n",
    "    edge_index = data.edge_index\n",
    "    mask = (node_mapping[edge_index[0]] >= 0) & (node_mapping[edge_index[1]] >= 0)\n",
    "    new_edge_index = node_mapping[edge_index[:, mask]]\n",
    "    \n",
    "    # Create new data object\n",
    "    new_data = data.__class__()\n",
    "    new_data.x = data.x[sampled_nodes]\n",
    "    new_data.edge_index = new_edge_index\n",
    "    new_data.y = data.y[sampled_nodes]\n",
    "    \n",
    "    return new_data, sampled_nodes\n",
    "\n",
    "print(\"Creating 500K subsample...\")\n",
    "subsampled_data, sampled_nodes = create_subsample(data, num_nodes=500000, seed=SEED)\n",
    "\n",
    "print(f\"\\nSubsampled dataset:\")\n",
    "print(f\"  Nodes: {subsampled_data.num_nodes:,}\")\n",
    "print(f\"  Edges: {subsampled_data.num_edges:,}\")\n",
    "print(f\"  Edge density: {subsampled_data.num_edges / (subsampled_data.num_nodes ** 2) * 100:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to labels 0-15 (excluding 4) - consistent with previous models\n",
    "def filter_labels(data, valid_labels):\n",
    "    \"\"\"\n",
    "    Filter data to only include specified labels.\n",
    "    This maintains consistency with MLP, GCN, and GraphSAGE models.\n",
    "    \"\"\"\n",
    "    y = data.y.squeeze()\n",
    "    mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "    for label in valid_labels:\n",
    "        mask |= (y == label)\n",
    "    \n",
    "    # Filter nodes\n",
    "    valid_node_idx = torch.where(mask)[0]\n",
    "    node_mapping = torch.full((len(y),), -1, dtype=torch.long)\n",
    "    node_mapping[valid_node_idx] = torch.arange(len(valid_node_idx))\n",
    "    \n",
    "    # Filter edges\n",
    "    edge_mask = mask[data.edge_index[0]] & mask[data.edge_index[1]]\n",
    "    new_edge_index = node_mapping[data.edge_index[:, edge_mask]]\n",
    "    \n",
    "    # Remap labels to 0-14\n",
    "    label_mapping = {label: idx for idx, label in enumerate(valid_labels)}\n",
    "    new_y = torch.tensor([label_mapping[y[i].item()] for i in valid_node_idx])\n",
    "    \n",
    "    # Create filtered data\n",
    "    filtered_data = data.__class__()\n",
    "    filtered_data.x = data.x[valid_node_idx]\n",
    "    filtered_data.edge_index = new_edge_index\n",
    "    filtered_data.y = new_y.unsqueeze(1)\n",
    "    \n",
    "    return filtered_data, valid_node_idx\n",
    "\n",
    "# Use labels 0-15 except 4 (same as other models)\n",
    "valid_labels = [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "print(f\"Filtering to {len(valid_labels)} labels...\")\n",
    "filtered_data, valid_nodes = filter_labels(subsampled_data, valid_labels)\n",
    "\n",
    "print(f\"\\nFiltered dataset:\")\n",
    "print(f\"  Nodes: {filtered_data.num_nodes:,}\")\n",
    "print(f\"  Edges: {filtered_data.num_edges:,}\")\n",
    "print(f\"  Classes: {len(valid_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test splits using sales_ranking (same as other models)\n",
    "split_idx = dataset.get_idx_split()\n",
    "\n",
    "# Map original indices to subsampled and filtered indices\n",
    "def map_split_indices(original_indices, sampled_nodes, valid_nodes):\n",
    "    \"\"\"Map original dataset indices to filtered subsample indices.\"\"\"\n",
    "    # Find which sampled nodes are in the split\n",
    "    mask = torch.isin(sampled_nodes, original_indices)\n",
    "    split_in_sample = sampled_nodes[mask]\n",
    "    \n",
    "    # Find which of these are in valid_nodes (after label filtering)\n",
    "    valid_mask = torch.isin(split_in_sample, sampled_nodes[valid_nodes])\n",
    "    final_split = split_in_sample[valid_mask]\n",
    "    \n",
    "    # Map to new indices\n",
    "    node_to_idx = {node.item(): idx for idx, node in enumerate(sampled_nodes[valid_nodes])}\n",
    "    mapped_indices = torch.tensor([node_to_idx[node.item()] for node in final_split])\n",
    "    \n",
    "    return mapped_indices\n",
    "\n",
    "train_idx = map_split_indices(split_idx['train'], sampled_nodes, valid_nodes)\n",
    "val_idx = map_split_indices(split_idx['valid'], sampled_nodes, valid_nodes)\n",
    "test_idx = map_split_indices(split_idx['test'], sampled_nodes, valid_nodes)\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"  Train: {len(train_idx):,} nodes ({len(train_idx)/filtered_data.num_nodes*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_idx):,} nodes ({len(val_idx)/filtered_data.num_nodes*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_idx):,} nodes ({len(test_idx)/filtered_data.num_nodes*100:.1f}%)\")\n",
    "\n",
    "# Create masks\n",
    "train_mask = torch.zeros(filtered_data.num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(filtered_data.num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(filtered_data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "filtered_data.train_mask = train_mask\n",
    "filtered_data.val_mask = val_mask\n",
    "filtered_data.test_mask = test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GraphSAINT Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAINT(torch.nn.Module):\n",
    "    \"\"\"GraphSAINT model using SAGEConv layers.\n",
    "    \n",
    "    GraphSAINT samples subgraphs before training, then trains on complete subgraphs.\n",
    "    This is more memory-efficient than GraphSAGE's per-node neighbor sampling.\n",
    "    \n",
    "    Architecture matches GCN and GraphSAGE for fair comparison:\n",
    "    - 2 SAGEConv layers\n",
    "    - 128 hidden channels\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "# Model parameters (consistent with other models)\n",
    "in_channels = filtered_data.x.shape[1]\n",
    "hidden_channels = 128\n",
    "out_channels = len(valid_labels)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"  Input: {in_channels} features\")\n",
    "print(f\"  Hidden: {hidden_channels} channels\")\n",
    "print(f\"  Output: {out_channels} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GraphSAINT Samplers\n",
    "\n",
    "GraphSAINT offers three sampling strategies:\n",
    "\n",
    "1. **Random Walk Sampling**: Samples subgraphs via random walks (usually best performance)\n",
    "2. **Node Sampling**: Randomly samples nodes and their induced subgraph\n",
    "3. **Edge Sampling**: Randomly samples edges and their incident nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samplers(data, sampling_strategy='random_walk'):\n",
    "    \"\"\"\n",
    "    Create GraphSAINT sampler based on strategy.\n",
    "    \n",
    "    Args:\n",
    "        data: PyG Data object\n",
    "        sampling_strategy: 'random_walk', 'node', or 'edge'\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "    if sampling_strategy == 'random_walk':\n",
    "        print(\"Using Random Walk Sampling\")\n",
    "        train_loader = GraphSAINTRandomWalkSampler(\n",
    "            data,\n",
    "            batch_size=6000,      # nodes per subgraph\n",
    "            walk_length=2,        # random walk length\n",
    "            num_steps=30,         # number of subgraphs per epoch\n",
    "            sample_coverage=100,  # number of times each node should be sampled\n",
    "            save_dir=None\n",
    "        )\n",
    "        \n",
    "    elif sampling_strategy == 'node':\n",
    "        print(\"Using Node Sampling\")\n",
    "        train_loader = GraphSAINTNodeSampler(\n",
    "            data,\n",
    "            batch_size=6000,\n",
    "            num_steps=30,\n",
    "            sample_coverage=100,\n",
    "            save_dir=None\n",
    "        )\n",
    "        \n",
    "    elif sampling_strategy == 'edge':\n",
    "        print(\"Using Edge Sampling\")\n",
    "        train_loader = GraphSAINTEdgeSampler(\n",
    "            data,\n",
    "            batch_size=6000,\n",
    "            num_steps=30,\n",
    "            sample_coverage=100,\n",
    "            save_dir=None\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown sampling strategy: {sampling_strategy}\")\n",
    "    \n",
    "    # For validation and test, we don't need sampling\n",
    "    # We'll evaluate on the full graph\n",
    "    return train_loader\n",
    "\n",
    "# We'll test all three strategies\n",
    "SAMPLING_STRATEGIES = ['random_walk', 'node', 'edge']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    \"\"\"Train for one epoch using GraphSAINT sampling.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_nodes = 0\n",
    "    \n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(batch_data.x, batch_data.edge_index)\n",
    "        \n",
    "        # Only compute loss on training nodes in this batch\n",
    "        train_mask = batch_data.train_mask\n",
    "        loss = F.cross_entropy(out[train_mask], batch_data.y.squeeze()[train_mask])\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item() * train_mask.sum().item()\n",
    "        pred = out[train_mask].argmax(dim=1)\n",
    "        total_correct += (pred == batch_data.y.squeeze()[train_mask]).sum().item()\n",
    "        total_nodes += train_mask.sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / total_nodes\n",
    "    accuracy = total_correct / total_nodes\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, mask, device):\n",
    "    \"\"\"Evaluate on full graph (no sampling).\"\"\"\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "    \n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[mask], data.y.squeeze()[mask])\n",
    "    \n",
    "    pred = out[mask].argmax(dim=1)\n",
    "    accuracy = (pred == data.y.squeeze()[mask]).float().mean().item()\n",
    "    \n",
    "    return loss.item(), accuracy\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_topk_accuracy(model, data, mask, device, k_values=[1, 3, 5]):\n",
    "    \"\"\"Compute top-k accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "    \n",
    "    out = model(data.x, data.edge_index)\n",
    "    logits = out[mask]\n",
    "    targets = data.y.squeeze()[mask]\n",
    "    \n",
    "    topk_accs = {}\n",
    "    for k in k_values:\n",
    "        _, topk_pred = torch.topk(logits, k, dim=1)\n",
    "        correct = (topk_pred == targets.unsqueeze(1)).any(dim=1)\n",
    "        topk_accs[f'top{k}'] = correct.float().mean().item()\n",
    "    \n",
    "    return topk_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graphsaint(data, sampling_strategy, num_epochs=400, lr=0.01, patience=50):\n",
    "    \"\"\"\n",
    "    Train GraphSAINT model with specified sampling strategy.\n",
    "    \n",
    "    Args:\n",
    "        data: PyG Data object\n",
    "        sampling_strategy: 'random_walk', 'node', or 'edge'\n",
    "        num_epochs: maximum number of epochs\n",
    "        lr: learning rate\n",
    "        patience: early stopping patience\n",
    "    \n",
    "    Returns:\n",
    "        model, training history, results dictionary\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training GraphSAINT with {sampling_strategy} sampling\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create sampler\n",
    "    train_loader = create_samplers(data, sampling_strategy)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = GraphSAINT(\n",
    "        in_channels=data.x.shape[1],\n",
    "        hidden_channels=128,\n",
    "        out_channels=len(valid_labels),\n",
    "        dropout=0.5\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_acc = evaluate(model, data, data.val_mask, device)\n",
    "        test_loss, test_acc = evaluate(model, data, data.test_mask, device)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:3d} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f} | \"\n",
    "                  f\"Test Acc: {test_acc:.4f}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch} (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Final evaluation\n",
    "    _, train_acc = evaluate(model, data, data.train_mask, device)\n",
    "    _, val_acc = evaluate(model, data, data.val_mask, device)\n",
    "    _, test_acc = evaluate(model, data, data.test_mask, device)\n",
    "    \n",
    "    # Compute top-k accuracy\n",
    "    topk_results = compute_topk_accuracy(model, data, data.test_mask, device)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Best model at epoch {best_epoch}:\")\n",
    "    print(f\"  Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Acc:   {val_acc:.4f}\")\n",
    "    print(f\"  Test Acc:  {test_acc:.4f}\")\n",
    "    print(f\"  Top-1 Acc: {topk_results['top1']:.4f}\")\n",
    "    print(f\"  Top-3 Acc: {topk_results['top3']:.4f}\")\n",
    "    print(f\"  Top-5 Acc: {topk_results['top5']:.4f}\")\n",
    "    print(f\"  Training time: {training_time:.2f}s\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'model': f'GraphSAINT_{sampling_strategy}',\n",
    "        'sampling_strategy': sampling_strategy,\n",
    "        'test_accuracy': test_acc,\n",
    "        'val_accuracy': val_acc,\n",
    "        'train_accuracy': train_acc,\n",
    "        'best_epoch': best_epoch,\n",
    "        'training_time': training_time,\n",
    "        'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'top1_acc': topk_results['top1'],\n",
    "        'top3_acc': topk_results['top3'],\n",
    "        'top5_acc': topk_results['top5'],\n",
    "        'hidden_channels': 128,\n",
    "        'num_classes': len(valid_labels),\n",
    "        'num_nodes': data.num_nodes,\n",
    "        'num_edges': data.num_edges,\n",
    "        'random_seed': SEED\n",
    "    }\n",
    "    \n",
    "    return model, history, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train All Three Sampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAMPLING_STRATEGIES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m all_models = {}\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Train with each sampling strategy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m strategy \u001b[38;5;129;01min\u001b[39;00m \u001b[43mSAMPLING_STRATEGIES\u001b[49m:\n\u001b[32m      8\u001b[39m     model, history, results = train_graphsaint(\n\u001b[32m      9\u001b[39m         filtered_data,\n\u001b[32m     10\u001b[39m         sampling_strategy=strategy,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m         patience=\u001b[32m50\u001b[39m\n\u001b[32m     14\u001b[39m     )\n\u001b[32m     16\u001b[39m     all_results[strategy] = results\n",
      "\u001b[31mNameError\u001b[39m: name 'SAMPLING_STRATEGIES' is not defined"
     ]
    }
   ],
   "source": [
    "# Store results for all strategies\n",
    "all_results = {}\n",
    "all_histories = {}\n",
    "all_models = {}\n",
    "\n",
    "# Train with each sampling strategy\n",
    "for strategy in SAMPLING_STRATEGIES:\n",
    "    model, history, results = train_graphsaint(\n",
    "        filtered_data,\n",
    "        sampling_strategy=strategy,\n",
    "        num_epochs=400,\n",
    "        lr=0.01,\n",
    "        patience=50\n",
    "    )\n",
    "    \n",
    "    all_results[strategy] = results\n",
    "    all_histories[strategy] = history\n",
    "    all_models[strategy] = model\n",
    "    \n",
    "    # Save individual results\n",
    "    with open(f'graphsaint_{strategy}_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to graphsaint_{strategy}_results.json\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Select Best Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all strategies\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON OF SAMPLING STRATEGIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for strategy in SAMPLING_STRATEGIES:\n",
    "    results = all_results[strategy]\n",
    "    print(f\"\\n{strategy.upper()} Sampling:\")\n",
    "    print(f\"  Test Accuracy: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"  Val Accuracy:  {results['val_accuracy']:.4f}\")\n",
    "    print(f\"  Top-3 Accuracy: {results['top3_acc']:.4f}\")\n",
    "    print(f\"  Training Time: {results['training_time']:.2f}s\")\n",
    "    print(f\"  Best Epoch:    {results['best_epoch']}\")\n",
    "\n",
    "# Select best based on validation accuracy\n",
    "best_strategy = max(all_results.items(), key=lambda x: x[1]['val_accuracy'])[0]\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BEST STRATEGY: {best_strategy.upper()} (Val Acc: {all_results[best_strategy]['val_accuracy']:.4f})\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Use best strategy for final results\n",
    "best_model = all_models[best_strategy]\n",
    "best_history = all_histories[best_strategy]\n",
    "best_results = all_results[best_strategy]\n",
    "\n",
    "# Save best results\n",
    "with open('graphsaint_best_results.json', 'w') as f:\n",
    "    json.dump(best_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison with Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from other models for comparison\n",
    "try:\n",
    "    with open('mlp_500k_results.json', 'r') as f:\n",
    "        mlp_results = json.load(f)\n",
    "    mlp_test_acc = mlp_results['test_accuracy']\n",
    "except:\n",
    "    mlp_test_acc = 0.6192  # Use your reported value\n",
    "\n",
    "try:\n",
    "    with open('gcn_results.json', 'r') as f:\n",
    "        gcn_results = json.load(f)\n",
    "    gcn_test_acc = gcn_results['test_accuracy']\n",
    "except:\n",
    "    gcn_test_acc = 0.7668  # Use your reported value\n",
    "\n",
    "try:\n",
    "    with open('GraphSage_results.json', 'r') as f:\n",
    "        sage_results = json.load(f)\n",
    "    sage_test_acc = sage_results['test_accuracy']\n",
    "except:\n",
    "    sage_test_acc = 0.7606  # Use your reported value\n",
    "\n",
    "# Add comparison metrics to best results\n",
    "best_results['mlp_baseline'] = mlp_test_acc\n",
    "best_results['gcn_baseline'] = gcn_test_acc\n",
    "best_results['graphsage_baseline'] = sage_test_acc\n",
    "best_results['improvement_over_mlp'] = best_results['test_accuracy'] - mlp_test_acc\n",
    "best_results['improvement_over_mlp_pct'] = (best_results['improvement_over_mlp'] / mlp_test_acc) * 100\n",
    "best_results['improvement_over_gcn'] = best_results['test_accuracy'] - gcn_test_acc\n",
    "best_results['improvement_over_gcn_pct'] = (best_results['improvement_over_gcn'] / gcn_test_acc) * 100\n",
    "best_results['improvement_over_sage'] = best_results['test_accuracy'] - sage_test_acc\n",
    "best_results['improvement_over_sage_pct'] = (best_results['improvement_over_sage'] / sage_test_acc) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nMLP Baseline:        {mlp_test_acc:.4f}\")\n",
    "print(f\"GCN:                 {gcn_test_acc:.4f} (+{(gcn_test_acc-mlp_test_acc)/mlp_test_acc*100:.2f}% vs MLP)\")\n",
    "print(f\"GraphSAGE:           {sage_test_acc:.4f} (+{(sage_test_acc-mlp_test_acc)/mlp_test_acc*100:.2f}% vs MLP)\")\n",
    "print(f\"GraphSAINT ({best_strategy}): {best_results['test_accuracy']:.4f} \"\n",
    "      f\"(+{best_results['improvement_over_mlp_pct']:.2f}% vs MLP)\")\n",
    "print(f\"\\nGraphSAINT vs GCN:       {best_results['improvement_over_gcn']:+.4f} ({best_results['improvement_over_gcn_pct']:+.2f}%)\")\n",
    "print(f\"GraphSAINT vs GraphSAGE: {best_results['improvement_over_sage']:+.4f} ({best_results['improvement_over_sage_pct']:+.2f}%)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Save final results with comparisons\n",
    "with open('graphsaint_best_results.json', 'w') as f:\n",
    "    json.dump(best_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Training curves for best strategy\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(best_history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(best_history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].plot(best_history['test_loss'], label='Test Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title(f'GraphSAINT ({best_strategy}) Training Loss Curves', fontsize=14)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(best_history['train_acc'], label='Train Acc', linewidth=2)\n",
    "axes[1].plot(best_history['val_acc'], label='Val Acc', linewidth=2)\n",
    "axes[1].plot(best_history['test_acc'], label='Test Acc', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title(f'GraphSAINT ({best_strategy}) Training Accuracy Curves', fontsize=14)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphsaint_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: graphsaint_training_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Top-k accuracy comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = ['MLP', 'GCN', 'GraphSAGE', f'GraphSAINT\\n({best_strategy})']\n",
    "top1_accs = [mlp_results.get('top1_acc', mlp_test_acc), \n",
    "             gcn_results.get('top1_acc', gcn_test_acc),\n",
    "             sage_results.get('top1_acc', sage_test_acc),\n",
    "             best_results['top1_acc']]\n",
    "top3_accs = [mlp_results.get('top3_acc', 0.85), \n",
    "             gcn_results.get('top3_acc', 0.93),\n",
    "             sage_results.get('top3_acc', 0.92),\n",
    "             best_results['top3_acc']]\n",
    "top5_accs = [mlp_results.get('top5_acc', 0.92), \n",
    "             gcn_results.get('top5_acc', 0.96),\n",
    "             sage_results.get('top5_acc', 0.96),\n",
    "             best_results['top5_acc']]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, top1_accs, width, label='Top-1', alpha=0.8)\n",
    "bars2 = ax.bar(x, top3_accs, width, label='Top-3', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, top5_accs, width, label='Top-5', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Top-k Accuracy Comparison: All Models', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0.5, 1.0])\n",
    "\n",
    "# Add value labels on bars\n",
    "def add_value_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "add_value_labels(bars1)\n",
    "add_value_labels(bars2)\n",
    "add_value_labels(bars3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphsaint_topk_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: graphsaint_topk_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Sampling strategy comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "strategies = [s.replace('_', ' ').title() for s in SAMPLING_STRATEGIES]\n",
    "test_accs = [all_results[s]['test_accuracy'] for s in SAMPLING_STRATEGIES]\n",
    "val_accs = [all_results[s]['val_accuracy'] for s in SAMPLING_STRATEGIES]\n",
    "train_times = [all_results[s]['training_time'] for s in SAMPLING_STRATEGIES]\n",
    "\n",
    "# Accuracy comparison\n",
    "x = np.arange(len(strategies))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, val_accs, width, label='Validation', alpha=0.8)\n",
    "bars2 = axes[0].bar(x + width/2, test_accs, width, label='Test', alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Sampling Strategy', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('GraphSAINT: Sampling Strategy Accuracy Comparison', fontsize=14)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(strategies)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_ylim([0.7, 0.9])\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.4f}',\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Training time comparison\n",
    "bars = axes[1].bar(strategies, train_times, alpha=0.8, color='coral')\n",
    "axes[1].set_xlabel('Sampling Strategy', fontsize=12)\n",
    "axes[1].set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "axes[1].set_title('GraphSAINT: Training Time Comparison', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}s',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphsaint_sampling_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: graphsaint_sampling_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Confusion Matrix\n",
    "@torch.no_grad()\n",
    "def compute_confusion_matrix(model, data, mask, device):\n",
    "    \"\"\"Compute confusion matrix.\"\"\"\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "    \n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out[mask].argmax(dim=1)\n",
    "    targets = data.y.squeeze()[mask]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    num_classes = out.shape[1]\n",
    "    conf_matrix = torch.zeros(num_classes, num_classes)\n",
    "    \n",
    "    for t, p in zip(targets, pred):\n",
    "        conf_matrix[t, p] += 1\n",
    "    \n",
    "    return conf_matrix.cpu().numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = compute_confusion_matrix(best_model, filtered_data, filtered_data.test_mask, device)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix, annot=False, fmt='g', cmap='Blues', ax=ax, cbar_kws={'label': 'Count'})\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "ax.set_title(f'GraphSAINT ({best_strategy}) Confusion Matrix on Test Set ({len(valid_labels)} Classes)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphsaint_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: graphsaint_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GRAPHSAINT IMPLEMENTATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nBest Sampling Strategy: {best_strategy.upper()}\")\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Test Accuracy:  {best_results['test_accuracy']:.4f}\")\n",
    "print(f\"  Top-3 Accuracy: {best_results['top3_acc']:.4f}\")\n",
    "print(f\"  Top-5 Accuracy: {best_results['top5_acc']:.4f}\")\n",
    "print(f\"  Training Time:  {best_results['training_time']:.2f}s\")\n",
    "\n",
    "print(f\"\\nComparison with Other Models:\")\n",
    "print(f\"  MLP:       {mlp_test_acc:.4f}\")\n",
    "print(f\"  GCN:       {gcn_test_acc:.4f}\")\n",
    "print(f\"  GraphSAGE: {sage_test_acc:.4f}\")\n",
    "print(f\"  GraphSAINT: {best_results['test_accuracy']:.4f} ‚≠ê\")\n",
    "\n",
    "print(f\"\\nFiles Generated:\")\n",
    "print(f\"  - graphsaint_best_results.json\")\n",
    "for strategy in SAMPLING_STRATEGIES:\n",
    "    print(f\"  - graphsaint_{strategy}_results.json\")\n",
    "print(f\"  - graphsaint_training_curves.png\")\n",
    "print(f\"  - graphsaint_topk_comparison.png\")\n",
    "print(f\"  - graphsaint_sampling_comparison.png\")\n",
    "print(f\"  - graphsaint_confusion_matrix.png\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. Edge Quality Sensitivity Analysis\")\n",
    "print(\"   - Test robustness to edge removal (sparsification)\")\n",
    "print(\"   - Test robustness to edge noise (random edge addition)\")\n",
    "print(\"   - Compare: GCN vs GraphSAGE vs GraphSAINT\")\n",
    "\n",
    "print(\"\\n2. Cold-Start Resilience Analysis\")\n",
    "print(\"   - Evaluate performance by node degree\")\n",
    "print(\"   - Test classification with edge removal (new products)\")\n",
    "print(\"   - Identify threshold where GNNs become beneficial\")\n",
    "\n",
    "print(\"\\n3. Final Report/Blog Post\")\n",
    "print(\"   - Synthesize all results\")\n",
    "print(\"   - Create comprehensive visualizations\")\n",
    "print(\"   - Write up insights and conclusions\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
